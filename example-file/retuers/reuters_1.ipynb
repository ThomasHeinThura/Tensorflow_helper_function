{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These reuters and imdb dataset are abit tricky to minpulate the data and hard to train with lstm, gru and bilateral lstm models\n",
    "I think that it is becuase the input size are too heavy and big it almost take 30 min and more ram with bilateral lstm models.\n",
    "I think I have to try with transformer model and also I need take and try with plain text dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from datetime import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.autograph.set_verbosity(0)\n",
    "tf.set_seed = 42\n",
    "epoch = 10\n",
    "max_vocab_length = 10000 # max number of words to have in our vocabulary\n",
    "max_length = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_features : (8982,) object \n",
      "Train_labels : (8982,) int64 \n",
      "Test features : (2246,) object \n",
      "Test_labels : (2246,) int64 \n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "(train_features,train_labels), (test_features, test_labels) = tf.keras.datasets.reuters.load_data(num_words=max_vocab_length)\n",
    "\n",
    "# Check the data shape\n",
    "print(\n",
    "    f\"Train_features : {train_features.shape} {train_features.dtype} \\n\" \n",
    "    f\"Train_labels : {train_labels.shape} {train_labels.dtype} \\n\" \n",
    "    f\"Test features : {test_features.shape} {test_features.dtype} \\n\" \n",
    "    f\"Test_labels : {test_labels.shape} {test_labels.dtype} \"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[1.000e+00 3.267e+03 6.990e+02 3.434e+03 2.295e+03 5.600e+01 2.000e+00\n",
      " 7.511e+03 9.000e+00 5.600e+01 3.906e+03 1.073e+03 8.100e+01 5.000e+00\n",
      " 1.198e+03 5.700e+01 3.660e+02 7.370e+02 1.320e+02 2.000e+01 4.093e+03\n",
      " 7.000e+00 2.000e+00 4.900e+01 2.295e+03 2.000e+00 1.037e+03 3.267e+03\n",
      " 6.990e+02 3.434e+03 8.000e+00 7.000e+00 1.000e+01 2.410e+02 1.600e+01\n",
      " 8.550e+02 1.290e+02 2.310e+02 7.830e+02 5.000e+00 4.000e+00 5.870e+02\n",
      " 2.295e+03 2.000e+00 2.000e+00 7.750e+02 7.000e+00 4.800e+01 3.400e+01\n",
      " 1.910e+02 4.400e+01 3.500e+01 1.795e+03 5.050e+02 1.700e+01 1.200e+01], shape=(56,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tensor_test = tf.cast(train_features[1], dtype=tf.float32)\n",
    "\n",
    "print(tensor_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VECTORIZE function\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=max_vocab_length):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return tf.cast(results, dtype= tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train features  (8982, 10000)\n",
      "test features  (2246, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Vectorize and Normalize train and test to tensors with 10k columns\n",
    "\n",
    "train_features_tf = vectorize_sequences(train_features)\n",
    "test_features_tf = vectorize_sequences(test_features)\n",
    "\n",
    "print(\"train features \", train_features_tf.shape)\n",
    "print(\"test features \", test_features_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_hot_train_labels  (8982, 46)\n",
      "one_hot_test_labels  (2246, 46)\n"
     ]
    }
   ],
   "source": [
    "# ONE HOT ENCODER of the labels\n",
    "\n",
    "one_hot_train_labels = tf.keras.utils.to_categorical(train_labels)\n",
    "one_hot_test_labels = tf.keras.utils.to_categorical(test_labels)\n",
    "\n",
    "print(\"one_hot_train_labels \", one_hot_train_labels.shape)\n",
    "print(\"one_hot_test_labels \", one_hot_test_labels.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : <PrefetchDataset element_spec=(TensorSpec(shape=(None, 10000), dtype=tf.float32, name=None), TensorSpec(shape=(None, 46), dtype=tf.float32, name=None))> \n",
      "Test : <PrefetchDataset element_spec=(TensorSpec(shape=(None, 10000), dtype=tf.float32, name=None), TensorSpec(shape=(None, 46), dtype=tf.float32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_features_tf, one_hot_train_labels))\n",
    "train_dataset =  train_dataset.shuffle(8982).batch(128).cache().prefetch(tf.data.AUTOTUNE)\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((test_features_tf,one_hot_test_labels))\n",
    "valid_dataset = valid_dataset.batch(128).cache().prefetch(tf.data.AUTOTUNE)\n",
    "print(f\"Train : {train_dataset} \\n\"\n",
    "      f\"Test : {valid_dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", # watch the val loss metric\n",
    "                                                  patience=5) # if val loss decreases for 3 epochs in a row, stop training\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",  \n",
    "                                                 factor=0.2, # multiply the learning rate by 0.2 (reduce by 5x)\n",
    "                                                 patience=3,\n",
    "                                                 verbose=1, # print out when learning rate goes down \n",
    "                                                 min_lr=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed and create embedding layer (new embedding layer for each model)\n",
    "tf.random.set_seed(42)\n",
    "from tensorflow.keras import layers\n",
    "embedding_layers = layers.Embedding(input_dim=max_vocab_length,\n",
    "                                     output_dim=2,\n",
    "                                     embeddings_initializer=\"uniform\",\n",
    "                                     input_length = max_length,\n",
    "                                     name=\"embedding_layers\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4_Bidirectional\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 10000)]           0         \n",
      "                                                                 \n",
      " embedding_layers (Embedding  (None, 10000, 2)         20000     \n",
      " )                                                               \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 128)              34304     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 46)                5934      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 60,238\n",
      "Trainable params: 60,238\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build a Bidirectional RNN in TensorFlow\n",
    "inputs = layers.Input(shape=(max_vocab_length,))\n",
    "x = embedding_layers(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(64))(x) # stacking RNN layers requires return_sequences=True\n",
    "#x = layers.Bidirectional(layers.LSTM(64))(x) # bidirectional goes both ways so has double the parameters of a regular LSTM layer\n",
    "outputs = layers.Dense(46, activation=\"sigmoid\")(x)\n",
    "model_1 = tf.keras.Model(inputs, outputs, name=\"model_4_Bidirectional\")\n",
    "\n",
    "# Compile\n",
    "model_1.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# Get a summary of our bidirectional model\n",
    "model_1.summary()\n",
    "\n",
    "# Fit the model (takes longer because of the bidirectional layers)\n",
    "#model_1_history = model_1.fit(train_dataset,\n",
    "#                           epochs=5,\n",
    "#                           validation_data=valid_dataset,\n",
    "#                           callbacks=[early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4_Bidirectional\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 10000)]           0         \n",
      "                                                                 \n",
      " embedding_layers (Embedding  (None, 10000, 2)         20000     \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 10000, 32)         352       \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 10000, 64)         10304     \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 10000, 128)        41088     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1280000)           0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 46)                58880046  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 58,951,790\n",
      "Trainable params: 58,951,790\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = layers.Input(shape=(max_vocab_length,))\n",
    "x = embedding_layers(inputs)\n",
    "x = layers.Conv1D(32, kernel_size=5, padding=\"same\", activation=\"relu\")(x)\n",
    "x = layers.Conv1D(64, kernel_size=5, padding=\"same\", activation=\"relu\")(x)\n",
    "x = layers.Conv1D(128, kernel_size=5, padding=\"same\", activation=\"relu\")(x)\n",
    "x = layers.Flatten()(x) # condense the output of our feature vector\n",
    "outputs = layers.Dense(46, activation=\"softmax\")(x)\n",
    "model_1= tf.keras.Model(inputs, outputs, name=\"model_4_Bidirectional\")\n",
    "\n",
    "# Compile\n",
    "model_1.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "model_1.summary()\n",
    "\n",
    "# start = datetime.now()\n",
    "# model_1_history = model_1.fit(train_dataset,\n",
    "#                            epochs=5,\n",
    "#                            validation_data=valid_dataset,\n",
    "#                            callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# end = datetime.now()\n",
    "# print(f\"The time taken to fit the modle is {end - start}\")\n",
    "# model_1.evaluate(valid_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hub and USE emcode layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "(train_features,train_labels), (test_features, test_labels) = tf.keras.datasets.reuters.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30979"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = tf.keras.datasets.reuters.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "len(reverse_word_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[32, 748, 125, 458, 987, 361]]\n",
      "[[32, 748, 125, 458, 987, 361], [42, 344, 145, 448, 187, 304]]\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list\n",
    "my_list = []\n",
    "values1 = [32, 748, 125, 458, 987, 361]\n",
    "my_list.append(values1)\n",
    "print(my_list)\n",
    "\n",
    "values2 = [42, 344, 145, 448, 187, 304]    \n",
    "my_list.append(values2)\n",
    "print(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_tokenized_sentence(sentences_array):\n",
    "    decode_sent = []\n",
    "    for values in train_features:\n",
    "        decoded_newswire = [' '.join([reverse_word_index.get( i - 3, '?') for i in values ]) ]\n",
    "        decode_sent.append(decoded_newswire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([8982])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sent_squeeze = tf.squeeze(decode_sent)\n",
    "decode_sent_squeeze.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"? generale de banque sa lt genb br and lt heller overseas corp of chicago have each taken 50 pct stakes in factoring company sa belgo factors generale de banque said in a statement it gave no financial details of the transaction sa belgo factors' turnover in 1986 was 17 5 billion belgian francs reuter 3\"]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sent[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([8982])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sent_tf = tf.constant(decode_sent_squeeze)\n",
    "decode_sent_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow_hub.keras_layer.KerasLayer at 0x7f53f16b39a0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "# Set random seed and create embedding layer (new embedding layer for each model)\n",
    "sentence_encoder_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "                                        input_shape=[], # shape of inputs coming to our model \n",
    "                                        dtype=tf.string, # data type of inputs coming to the USE layer\n",
    "                                        trainable=False, # keep the pretrained weights (we'll create a feature extractor)\n",
    "                                        name=\"USE\")\n",
    "\n",
    "sentence_encoder_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_USE\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " USE (KerasLayer)            (None, 512)               256797824 \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                32832     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 46)                2990      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 256,833,646\n",
      "Trainable params: 35,822\n",
      "Non-trainable params: 256,797,824\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#BUild USE model \n",
    "# inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "# x = sentence_encoder_layer(inputs)\n",
    "# x = layers.Conv1D(32, kernel_size=5, padding=\"same\", activation=\"relu\")(x)\n",
    "# x = layers.Conv1D(64, kernel_size=5, padding=\"same\", activation=\"relu\")(x)\n",
    "# x = layers.Conv1D(128, kernel_size=5, padding=\"same\", activation=\"relu\")(x)\n",
    "# x = layers.Flatten()(x) # condense the output of our feature vector\n",
    "# outputs = layers.Dense(46, activation=\"softmax\")(x)\n",
    "# model_USE= tf.keras.Model(inputs, outputs, name=\"USE\")\n",
    "\n",
    "model_USE = tf.keras.Sequential([\n",
    "  sentence_encoder_layer, # take in sentences and then encode them into an embedding\n",
    "  layers.Dense(64, activation=\"relu\"),\n",
    "  layers.Dense(46, activation=\"sigmoid\")\n",
    "], name=\"model_USE\")\n",
    "\n",
    "# Compile\n",
    "model_USE.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "model_USE.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "281/281 [==============================] - ETA: 0s - loss: 1.8940 - accuracy: 0.5815"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/hanlinn/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1820, in test_function  *\n        return step_function(self, iterator)\n    File \"/home/hanlinn/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1804, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/hanlinn/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1792, in run_step  **\n        outputs = model.test_step(data)\n    File \"/home/hanlinn/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1756, in test_step\n        y_pred = self(x, training=False)\n    File \"/home/hanlinn/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filemaiizif1.py\", line 74, in tf__call\n        ag__.if_stmt(ag__.not_(ag__.ld(self)._has_training_argument), if_body_3, else_body_3, get_state_3, set_state_3, ('result', 'training'), 1)\n    File \"/tmp/__autograph_generated_filemaiizif1.py\", line 37, in if_body_3\n        result = ag__.converted_call(ag__.ld(f), (), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'USE' (type KerasLayer).\n    \n    in user code:\n    \n        File \"/home/hanlinn/00.projects/tensorflow-prepare/venv/lib/python3.10/site-packages/tensorflow_hub/keras_layer.py\", line 229, in call  *\n            result = f()\n    \n        ValueError: Shape must be rank 1 but is rank 2 for '{{node text_preprocessor/tokenize/StringSplit/StringSplit}} = StringSplit[skip_empty=true](text_preprocessor/StaticRegexReplace_1, text_preprocessor/tokenize/StringSplit/Const)' with input shapes: [?,10000], [].\n    \n    \n    Call arguments received by layer 'USE' (type KerasLayer):\n      • inputs=tf.Tensor(shape=(None, 10000), dtype=string)\n      • training=False\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m start \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow()\n\u001b[0;32m----> 2\u001b[0m model_USE_history \u001b[39m=\u001b[39m model_USE\u001b[39m.\u001b[39;49mfit(decode_sent_tf,\n\u001b[1;32m      3\u001b[0m                                   one_hot_train_labels,\n\u001b[1;32m      4\u001b[0m                                   epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m                                   validation_data\u001b[39m=\u001b[39;49mvalid_dataset,\n\u001b[1;32m      6\u001b[0m                                   callbacks\u001b[39m=\u001b[39;49m[early_stopping, reduce_lr])\n\u001b[1;32m      8\u001b[0m end \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow()\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe time taken to fit the modle is \u001b[39m\u001b[39m{\u001b[39;00mend \u001b[39m-\u001b[39m start\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filexx4mkihx.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__test_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filemaiizif1.py:74\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m     72\u001b[0m     result \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(smart_cond)\u001b[39m.\u001b[39msmart_cond, (ag__\u001b[39m.\u001b[39mld(training), ag__\u001b[39m.\u001b[39mautograph_artifact(\u001b[39mlambda\u001b[39;00m : ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(f), (), \u001b[39mdict\u001b[39m(training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m), fscope)), ag__\u001b[39m.\u001b[39mautograph_artifact(\u001b[39mlambda\u001b[39;00m : ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(f), (), \u001b[39mdict\u001b[39m(training\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m), fscope))), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     73\u001b[0m result \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mresult\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 74\u001b[0m ag__\u001b[39m.\u001b[39;49mif_stmt(ag__\u001b[39m.\u001b[39;49mnot_(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m_has_training_argument), if_body_3, else_body_3, get_state_3, set_state_3, (\u001b[39m'\u001b[39;49m\u001b[39mresult\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mtraining\u001b[39;49m\u001b[39m'\u001b[39;49m), \u001b[39m1\u001b[39;49m)\n\u001b[1;32m     76\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state_6\u001b[39m():\n\u001b[1;32m     77\u001b[0m     \u001b[39mreturn\u001b[39;00m (result,)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filemaiizif1.py:37\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.if_body_3\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mif_body_3\u001b[39m():\n\u001b[1;32m     36\u001b[0m     \u001b[39mnonlocal\u001b[39;00m result, training\n\u001b[0;32m---> 37\u001b[0m     result \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(f), (), \u001b[39mNone\u001b[39;49;00m, fscope)\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/hanlinn/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1820, in test_function  *\n        return step_function(self, iterator)\n    File \"/home/hanlinn/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1804, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/hanlinn/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1792, in run_step  **\n        outputs = model.test_step(data)\n    File \"/home/hanlinn/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1756, in test_step\n        y_pred = self(x, training=False)\n    File \"/home/hanlinn/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filemaiizif1.py\", line 74, in tf__call\n        ag__.if_stmt(ag__.not_(ag__.ld(self)._has_training_argument), if_body_3, else_body_3, get_state_3, set_state_3, ('result', 'training'), 1)\n    File \"/tmp/__autograph_generated_filemaiizif1.py\", line 37, in if_body_3\n        result = ag__.converted_call(ag__.ld(f), (), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'USE' (type KerasLayer).\n    \n    in user code:\n    \n        File \"/home/hanlinn/00.projects/tensorflow-prepare/venv/lib/python3.10/site-packages/tensorflow_hub/keras_layer.py\", line 229, in call  *\n            result = f()\n    \n        ValueError: Shape must be rank 1 but is rank 2 for '{{node text_preprocessor/tokenize/StringSplit/StringSplit}} = StringSplit[skip_empty=true](text_preprocessor/StaticRegexReplace_1, text_preprocessor/tokenize/StringSplit/Const)' with input shapes: [?,10000], [].\n    \n    \n    Call arguments received by layer 'USE' (type KerasLayer):\n      • inputs=tf.Tensor(shape=(None, 10000), dtype=string)\n      • training=False\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "model_USE_history = model_USE.fit(decode_sent_tf,\n",
    "                                  one_hot_train_labels,\n",
    "                                  epochs=5,\n",
    "                                  validation_data=valid_dataset,\n",
    "                                  callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "end = datetime.now()\n",
    "print(f\"The time taken to fit the modle is {end - start}\")\n",
    "model_USE.evaluate(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2fcd31d782b6120f2993dc67dd18a19e8000177e7f61203562e17b1bd19b8f69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
