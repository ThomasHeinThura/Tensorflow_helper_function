{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-02 00:29:45.998116: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.autograph.set_verbosity(0)\n",
    "tf.set_seed = 42\n",
    "epoch = 10\n",
    "max_vocab_length = 10000 # max number of words to have in our vocabulary\n",
    "max_length = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3  4  3 ... 25  3 25]\n",
      "[ 3 10  1 ...  3  3 24]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import data\n",
    "train_data, test_data = tf.keras.datasets.reuters.load_data(num_words=max_vocab_length)\n",
    "\n",
    "\"\"\"\n",
    "# Check the data shape\n",
    "print(\n",
    "    f\"Train_features : {train_features.shape} {train_features.dtype} \\n\" \n",
    "    f\"Train_labels : {train_labels.shape} {train_labels.dtype} \\n\" \n",
    "    f\"Test features : {test_features.shape} {test_features.dtype} \\n\" \n",
    "    f\"Test_labels : {test_labels.shape} {test_labels.dtype} \"\n",
    "    )\n",
    "\"\"\"\n",
    "print(train_data[1])\n",
    "print( test_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]),\n",
       "       list([1, 3267, 699, 3434, 2295, 56, 2, 7511, 9, 56, 3906, 1073, 81, 5, 1198, 57, 366, 737, 132, 20, 4093, 7, 2, 49, 2295, 2, 1037, 3267, 699, 3434, 8, 7, 10, 241, 16, 855, 129, 231, 783, 5, 4, 587, 2295, 2, 2, 775, 7, 48, 34, 191, 44, 35, 1795, 505, 17, 12]),\n",
       "       list([1, 53, 12, 284, 15, 14, 272, 26, 53, 959, 32, 818, 15, 14, 272, 26, 39, 684, 70, 11, 14, 12, 3886, 18, 180, 183, 187, 70, 11, 14, 102, 32, 11, 29, 53, 44, 704, 15, 14, 19, 758, 15, 53, 959, 47, 1013, 15, 14, 19, 132, 15, 39, 965, 32, 11, 14, 147, 72, 11, 180, 183, 187, 44, 11, 14, 102, 19, 11, 123, 186, 90, 67, 960, 4, 78, 13, 68, 467, 511, 110, 59, 89, 90, 67, 1390, 55, 2678, 92, 617, 80, 1274, 46, 905, 220, 13, 4, 346, 48, 235, 629, 5, 211, 5, 1118, 7, 2, 81, 5, 187, 11, 15, 9, 1709, 201, 5, 47, 3615, 18, 478, 4514, 5, 1118, 7, 232, 2, 71, 5, 160, 63, 11, 9, 2, 81, 5, 102, 59, 11, 17, 12]),\n",
       "       ...,\n",
       "       list([1, 141, 3890, 387, 81, 8, 16, 1629, 10, 340, 1241, 850, 31, 56, 3890, 691, 9, 1241, 71, 9, 5985, 2, 2, 699, 2, 2, 2, 699, 244, 5945, 4, 49, 8, 4, 656, 850, 33, 2993, 9, 2139, 340, 3371, 1493, 9, 2, 22, 2, 1094, 687, 83, 35, 15, 257, 6, 57, 9190, 7, 4, 5956, 654, 5, 2, 6191, 1371, 4, 49, 8, 16, 369, 646, 6, 1076, 7, 124, 407, 17, 12]),\n",
       "       list([1, 53, 46, 957, 26, 14, 74, 132, 26, 39, 46, 258, 3614, 18, 14, 74, 134, 5131, 18, 88, 2321, 72, 11, 14, 1842, 32, 11, 123, 383, 89, 39, 46, 235, 10, 864, 728, 5, 258, 44, 11, 15, 22, 753, 9, 42, 92, 131, 728, 5, 69, 312, 11, 15, 22, 222, 2, 3237, 383, 48, 39, 74, 235, 10, 864, 276, 5, 61, 32, 11, 15, 21, 4, 211, 5, 126, 1072, 42, 92, 131, 46, 19, 352, 11, 15, 22, 710, 220, 9, 42, 92, 131, 276, 5, 59, 61, 11, 15, 22, 10, 455, 7, 1172, 137, 336, 1325, 6, 1532, 142, 971, 6463, 43, 359, 5, 4, 326, 753, 364, 17, 12]),\n",
       "       list([1, 227, 2406, 91, 2, 125, 2855, 21, 4, 3976, 76, 7, 4, 757, 481, 3976, 790, 5259, 5654, 9, 111, 149, 8, 7, 10, 76, 223, 51, 4, 417, 8, 1047, 91, 6917, 1688, 340, 7, 194, 9411, 6, 1894, 21, 127, 2151, 2394, 1456, 6, 3034, 4, 329, 433, 7, 65, 87, 1127, 10, 8219, 1475, 290, 9, 21, 567, 16, 1926, 24, 4, 76, 209, 30, 4033, 6655, 5654, 8, 4, 60, 8, 4, 966, 308, 40, 2575, 129, 2, 295, 277, 1071, 9, 24, 286, 2114, 234, 222, 9, 4, 906, 3994, 8519, 114, 5758, 1752, 7, 4, 113, 17, 12])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feat_data, train_labels_data = train_data\n",
    "train_feat_data_a = train_feat_data \n",
    "train_feat_data_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_features  (8982, 10000)\n",
      "test_Features  (2246, 10000)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# VECTORIZE function\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=max_vocab_length):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return tf.cast(results, dtype= tf.float32)\n",
    "\n",
    "# Vectorize and Normalize train and test to tensors with 10k columns\n",
    "\n",
    "train_features_tf = vectorize_sequences(train_features)\n",
    "test_features_tf = vectorize_sequences(test_features)\n",
    "\n",
    "print(\"train_features \", train_features_tf.shape)\n",
    "print(\"test_Features \", test_features_tf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_hot_train_labels  (8982, 46)\n",
      "one_hot_test_labels  (2246, 46)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ONE HOT ENCODER of the labels\n",
    "\n",
    "one_hot_train_labels = tf.keras.utils.to_categorical(train_labels)\n",
    "one_hot_test_labels = tf.keras.utils.to_categorical(test_labels)\n",
    "\n",
    "print(\"one_hot_train_labels \", one_hot_train_labels.shape)\n",
    "print(\"one_hot_test_labels \", one_hot_test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : <PrefetchDataset element_spec=(TensorSpec(shape=(None, 10000), dtype=tf.float32, name=None), TensorSpec(shape=(None, 46), dtype=tf.float32, name=None))> \n",
      "Test : <PrefetchDataset element_spec=(TensorSpec(shape=(None, 10000), dtype=tf.float32, name=None), TensorSpec(shape=(None, 46), dtype=tf.float32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_features_tf, one_hot_train_labels))\n",
    "train_dataset =  train_dataset.shuffle(8982).batch(32).cache().prefetch(tf.data.AUTOTUNE)\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((test_features_tf,one_hot_test_labels))\n",
    "valid_dataset = valid_dataset.batch(32).cache().prefetch(tf.data.AUTOTUNE)\n",
    "print(f\"Train : {train_dataset} \\n\"\n",
    "      f\"Test : {valid_dataset}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Callbacks\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", # watch the val loss metric\n",
    "                                                  patience=5) # if val loss decreases for 3 epochs in a row, stop training\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",  \n",
    "                                                 factor=0.2, # multiply the learning rate by 0.2 (reduce by 5x)\n",
    "                                                 patience=3,\n",
    "                                                 verbose=1, # print out when learning rate goes down \n",
    "                                                 min_lr=1e-7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set random seed and create embedding layer (new embedding layer for each model)\n",
    "embedding_layers = layers.Embedding(input_dim=max_vocab_length,\n",
    "                                     output_dim=5,\n",
    "                                     embeddings_initializer=\"uniform\",\n",
    "                                     input_length = max_length,\n",
    "                                     name=\"embedding_layers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4_Bidirectional\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 10000)]           0         \n",
      "                                                                 \n",
      " embedding_layers (Embedding  (None, 10000, 5)         50000     \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 10000, 32)         832       \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 10000, 64)         10304     \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 10000, 128)        41088     \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 128)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 46)                5934      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 108,158\n",
      "Trainable params: 108,158\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build a Bidirectional RNN in TensorFlow\n",
    "inputs = layers.Input(shape=(max_vocab_length,))\n",
    "x = embedding_layers(inputs)\n",
    "x = layers.Conv1D(32, kernel_size=5, padding=\"same\", activation=\"relu\")(x)\n",
    "x = layers.Conv1D(64, kernel_size=5, padding=\"same\", activation=\"relu\")(x)\n",
    "x = layers.Conv1D(128, kernel_size=5, padding=\"same\", activation=\"relu\")(x)\n",
    "x = layers.GlobalAveragePooling1D()(x) # condense the output of our feature vector\n",
    "outputs = layers.Dense(46, activation=\"softmax\")(x)\n",
    "model= tf.keras.Model(inputs, outputs, name=\"model_4_Bidirectional\")\n",
    "\n",
    "# Compile\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# Get a summary of our bidirectional model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = datetime.now()\n",
    "# Fit the model (takes longer because of the bidirectional layers)\n",
    "model_history = model.fit(train_dataset,\n",
    "                           epochs=5,\n",
    "                           validation_data=valid_dataset,\n",
    "                           callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "end = datetime.now()\n",
    "print(f\"The time taken to fit the modle is {end - start}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(valid_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2fcd31d782b6120f2993dc67dd18a19e8000177e7f61203562e17b1bd19b8f69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
