# 1. for import data #
# 1.1 import from local data
def unzip_data(filename):
    import zipfile
    """
    Unzips filename into the current working directory.

    Args:
        filename (str): a filepath to a target zip folder to be unzipped.
    """
    zip_ref = zipfile.ZipFile(filename, "r")
    zip_ref.extractall()
    zip_ref.close()

# data work thorugh dirpath for CNN
def walk_through_dir(dir_path):
    import os 
    """
    Walks through dir_path returning its contents.

    Args:
        dir_path (str): target directory
  
    Returns:
        A print out of:
        number of subdiretories in dir_path
        number of images (files) in each subdirectory
        name of each subdirectory
    """
    for dirpath, dirnames, filenames in os.walk(dir_path):
        print(f"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.")

# Visualize classnes from dir
def view_class_name_from_dir(path):
    import pathlib 
    import numpy as np
    """
    View an class name from import data folder
    :param path:
    :return print class_name:
    """
    data_dir = pathlib.Path(path)
    class_name = np.array(sorted([item.name for item in data_dir.glob('*')]))
    return print(class_name)

# 1.2 import from tf.keras.dataset # Procedures
def show_methods_for_import_dataset():
    """
    import tensorflow as tf
    from tensorflow.keras.datasets import fashion_mnist

    # The data has already been sorted into training and test sets for us
    (train_data, train_labels), (test_data, test_labels) = fashion_mnist.load_data()

    # Viewing the single example for fashion_mnist
    # Plot a single example
    import matplotlib.pyplot as plt
    plt.imshow(train_data[7]);

    # Plot an example image and its label
    plt.imshow(train_data[17], cmap=plt.cm.binary) # change the colours to black & white
    plt.title(class_names[train_labels[17]]);
    """
    """ Import from dataset
    # Get TensorFlow Datasets
    import tensorflow_datasets as tfds
    # List available datasets
    datasets_list = tfds.list_builders() # get all available datasets in TFDS
    print("food101" in datasets_list) # is the dataset we're after available?

    # Load in the data (takes about 5-6 minutes in Google Colab)
    (train_data, test_data), ds_info = tfds.load(name="food101", # target dataset to get from TFDS
                                                 split=["train", "validation"], # what splits of data should we get? note: not all datasets have train, valid, test
                                                 shuffle_files=True, # shuffle files on download?
                                                 as_supervised=True, # download data in tuple format (sample, label), e.g. (image, label)
                                                 with_info=True) # include dataset metadata? if so, tfds.load() returns tuple (data, ds_info)
    """


# 1.2 import from tf.keras.dataset # Procedures
def show_methods_for_import_dataset():
  """
  import tensorflow as tf
  from tensorflow.keras.datasets import fashion_mnist

  # The data has already been sorted into training and test sets for us
  (train_data, train_labels), (test_data, test_labels) = fashion_mnist.load_data()

  # Viewing the single example for fashion_mnist
  # Plot a single example
  import matplotlib.pyplot as plt
  plt.imshow(train_data[7]);

  # Plot an example image and its label
  plt.imshow(train_data[17], cmap=plt.cm.binary) # change the colours to black & white
  plt.title(class_names[train_labels[17]]);
  """

# 1.3 improt from cvs
""" import from cvs:
import pandas as pd
from tensorflow import tf.datapath = ‘path/to/your/csv/file’
df = pd.read_csv(path)
dataset = Dataset.from_tensor_slices(dict(df))

OR

import from pandas
# Import with pandas 
import pandas as pd
# Parse dates and set date column to index
df = pd.read_csv("/content/BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv", 
                 parse_dates=["Date"], 
                 index_col=["Date"]) # parse the date column (tell pandas column 1 is a datetime)
df.head()
"""

# 1.4 import from online data
def show_methods_for_import_online():
  """
  This is the example procedure to do in cvs data
  import pandas as pd
  # Read in the insurance dataset
  insurance = pd.read_csv("https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv")
  # Check out the insurance dataset
  insurance.head()
  # Turn all categories into numbers
  insurance_one_hot = pd.get_dummies(insurance)
  insurance_one_hot.head() # view the converted columns
  # Create X & y values
  X = insurance_one_hot.drop("charges", axis=1)
  y = insurance_one_hot["charges"]
  # Create training and test sets
  from sklearn.model_selection import train_test_split
  X_train, X_test, y_train, y_test = train_test_split(X, 
                                                      y, 
                                                      test_size=0.2, 
                                                      random_state=42) 


  from sklearn.compose import make_column_transformer
  from sklearn.preprocessing import MinMaxScaler, OneHotEncoder

  # Create column transformer (this will help us normalize/preprocess our data)
  ct = make_column_transformer(
      (MinMaxScaler(), ["age", "bmi", "children"]), # get all values between 0 and 1
      (OneHotEncoder(handle_unknown="ignore"), ["sex", "smoker", "region"])
  )

  # Create X & y
  X = insurance.drop("charges", axis=1)
  y = insurance["charges"]

  # Build our train and test sets (use random state to ensure same split as before)
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

  # Fit column transformer on the training data only (doing so on test data would result in data leakage)
  ct.fit(X_train)

  # Transform training and test data with normalization (MinMaxScalar) and one hot encoding (OneHotEncoder)
  X_train_normal = ct.transform(X_train)
  X_test_normal = ct.transform(X_test)

  """