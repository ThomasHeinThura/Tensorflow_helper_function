{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These reuters and imdb dataset are abit tricky to minpulate the data and hard to train with lstm, gru and bilateral lstm models\n",
    "I think that it is becuase the input size are too heavy and big it almost take 30 min and more ram with bilateral lstm models.\n",
    "I think I have to try with transformer model and also I need take and try with plain text dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.autograph.set_verbosity(0)\n",
    "tf.set_seed = 42\n",
    "epoch = 10\n",
    "max_vocab_length = 10000 # max number of words to have in our vocabulary\n",
    "max_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_features : (8982,) object \n",
      "Train_labels : (8982,) int64 \n",
      "Test features : (2246,) object \n",
      "Test_labels : (2246,) int64 \n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "(train_features,train_labels), (test_features, test_labels) = tf.keras.datasets.reuters.load_data(num_words=max_vocab_length)\n",
    "\n",
    "# Check the data shape\n",
    "print(\n",
    "    f\"Train_features : {train_features.shape} {train_features.dtype} \\n\" \n",
    "    f\"Train_labels : {train_labels.shape} {train_labels.dtype} \\n\" \n",
    "    f\"Test features : {test_features.shape} {test_features.dtype} \\n\" \n",
    "    f\"Test_labels : {test_labels.shape} {test_labels.dtype} \"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[1.000e+00 3.267e+03 6.990e+02 3.434e+03 2.295e+03 5.600e+01 2.000e+00\n",
      " 7.511e+03 9.000e+00 5.600e+01 3.906e+03 1.073e+03 8.100e+01 5.000e+00\n",
      " 1.198e+03 5.700e+01 3.660e+02 7.370e+02 1.320e+02 2.000e+01 4.093e+03\n",
      " 7.000e+00 2.000e+00 4.900e+01 2.295e+03 2.000e+00 1.037e+03 3.267e+03\n",
      " 6.990e+02 3.434e+03 8.000e+00 7.000e+00 1.000e+01 2.410e+02 1.600e+01\n",
      " 8.550e+02 1.290e+02 2.310e+02 7.830e+02 5.000e+00 4.000e+00 5.870e+02\n",
      " 2.295e+03 2.000e+00 2.000e+00 7.750e+02 7.000e+00 4.800e+01 3.400e+01\n",
      " 1.910e+02 4.400e+01 3.500e+01 1.795e+03 5.050e+02 1.700e+01 1.200e+01], shape=(56,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tensor_test = tf.cast(train_features[1], dtype=tf.float32)\n",
    "\n",
    "print(tensor_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VECTORIZE function\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=max_vocab_length):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return tf.cast(results, dtype= tf.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train  (8982, 10000)\n",
      "x_test  (2246, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Vectorize and Normalize train and test to tensors with 10k columns\n",
    "\n",
    "train_features_tf = vectorize_sequences(train_features)\n",
    "test_features_tf = vectorize_sequences(test_features)\n",
    "\n",
    "print(\"x_train \", train_features_tf.shape)\n",
    "print(\"x_test \", test_features_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tf.float32, TensorShape([8982, 10000]), 2)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_tf.dtype , train_features_tf.shape, train_features_tf.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_hot_train_labels  (8982, 46)\n",
      "one_hot_test_labels  (2246, 46)\n"
     ]
    }
   ],
   "source": [
    "# ONE HOT ENCODER of the labels\n",
    "\n",
    "one_hot_train_labels = tf.keras.utils.to_categorical(train_labels)\n",
    "one_hot_test_labels = tf.keras.utils.to_categorical(test_labels)\n",
    "\n",
    "print(\"one_hot_train_labels \", one_hot_train_labels.shape)\n",
    "print(\"one_hot_test_labels \", one_hot_test_labels.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : <PrefetchDataset element_spec=(TensorSpec(shape=(None, 10000), dtype=tf.float32, name=None), TensorSpec(shape=(None, 46), dtype=tf.float32, name=None))> \n",
      "Test : <PrefetchDataset element_spec=(TensorSpec(shape=(None, 10000), dtype=tf.float32, name=None), TensorSpec(shape=(None, 46), dtype=tf.float32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_features_tf, one_hot_train_labels))\n",
    "train_dataset =  train_dataset.shuffle(8982).batch(128).cache().prefetch(tf.data.AUTOTUNE)\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((test_features_tf,one_hot_test_labels))\n",
    "valid_dataset = valid_dataset.batch(128).cache().prefetch(tf.data.AUTOTUNE)\n",
    "print(f\"Train : {train_dataset} \\n\"\n",
    "      f\"Test : {valid_dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", # watch the val loss metric\n",
    "                                                  patience=5) # if val loss decreases for 3 epochs in a row, stop training\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",  \n",
    "                                                 factor=0.2, # multiply the learning rate by 0.2 (reduce by 5x)\n",
    "                                                 patience=3,\n",
    "                                                 verbose=1, # print out when learning rate goes down \n",
    "                                                 min_lr=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed and create embedding layer (new embedding layer for each model)\n",
    "tf.random.set_seed(42)\n",
    "from tensorflow.keras import layers\n",
    "embedding_layers = layers.Embedding(input_dim=max_vocab_length,\n",
    "                                     output_dim=5,\n",
    "                                     embeddings_initializer=\"uniform\",\n",
    "                                     input_length = max_length,\n",
    "                                     name=\"embedding_layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_15 (InputLayer)       [(None, 10000)]           0         \n",
      "                                                                 \n",
      " embedding_layers (Embedding  multiple                 50000     \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 10000, 64)         1664      \n",
      "                                                                 \n",
      " global_average_pooling1d_1   (None, 64)               0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 46)                2990      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 54,654\n",
      "Trainable params: 54,654\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "281/281 [==============================] - 54s 191ms/step - loss: 2.7098 - accuracy: 0.3450 - val_loss: 2.4147 - val_accuracy: 0.3620 - lr: 0.0010\n",
      "Epoch 2/5\n",
      "281/281 [==============================] - 51s 180ms/step - loss: 2.4036 - accuracy: 0.3517 - val_loss: 2.4089 - val_accuracy: 0.3620 - lr: 0.0010\n",
      "Epoch 3/5\n",
      "281/281 [==============================] - 53s 189ms/step - loss: 2.3954 - accuracy: 0.3517 - val_loss: 2.3931 - val_accuracy: 0.3620 - lr: 0.0010\n",
      "Epoch 4/5\n",
      "281/281 [==============================] - 52s 183ms/step - loss: 2.3706 - accuracy: 0.3527 - val_loss: 2.3517 - val_accuracy: 0.3664 - lr: 0.0010\n",
      "Epoch 5/5\n",
      "281/281 [==============================] - 52s 184ms/step - loss: 2.3281 - accuracy: 0.3602 - val_loss: 2.3077 - val_accuracy: 0.3740 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "tf.set_seed = 42\n",
    "\n",
    "# Build a Bidirectional RNN in TensorFlow\n",
    "inputs = layers.Input(shape=(max_vocab_length,))\n",
    "embedding = embedding_layers(inputs)\n",
    "x = layers.Conv1D(64, kernel_size=5, padding=\"same\", activation=\"relu\")(embedding)\n",
    "x = layers.GlobalAveragePooling1D()(x) # condense the output of our feature vector\n",
    "outputs = layers.Dense(46, activation=\"softmax\")(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# Get a summary of our bidirectional model\n",
    "model.summary()\n",
    "\n",
    "# Fit the model (takes longer because of the bidirectional layers)\n",
    "model_history = model.fit(train_features_tf,\n",
    "                          one_hot_train_labels,\n",
    "                          epochs=5,\n",
    "                           validation_data=(test_features_tf,one_hot_test_labels),\n",
    "                           callbacks=[early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4_Bidirectional\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_16 (InputLayer)       [(None, 10000)]           0         \n",
      "                                                                 \n",
      " embedding_layers (Embedding  multiple                 50000     \n",
      " )                                                               \n",
      "                                                                 \n",
      " bidirectional_17 (Bidirecti  (None, 10000, 128)       35840     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_18 (Bidirecti  (None, 128)              98816     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 46)                5934      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 190,590\n",
      "Trainable params: 190,590\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "# Build a Bidirectional RNN in TensorFlow\n",
    "inputs = layers.Input(shape=(max_vocab_length,))\n",
    "x = embedding_layers(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x) # stacking RNN layers requires return_sequences=True\n",
    "x = layers.Bidirectional(layers.LSTM(64))(x) # bidirectional goes both ways so has double the parameters of a regular LSTM layer\n",
    "outputs = layers.Dense(46, activation=\"sigmoid\")(x)\n",
    "model= tf.keras.Model(inputs, outputs, name=\"model_4_Bidirectional\")\n",
    "\n",
    "# Compile\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# Get a summary of our bidirectional model\n",
    "model.summary()\n",
    "\n",
    "# Fit the model (takes longer because of the bidirectional layers)\n",
    "model_history = model.fit(train_dataset,\n",
    "                           epochs=5,\n",
    "                           validation_data=valid_dataset,\n",
    "                           callbacks=[early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Dec 19 2022, 17:35:49) [GCC 12.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2fcd31d782b6120f2993dc67dd18a19e8000177e7f61203562e17b1bd19b8f69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
