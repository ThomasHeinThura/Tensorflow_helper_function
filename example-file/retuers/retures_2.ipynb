{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-04 12:17:32.530518: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-04 12:17:32.904012: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-04 12:17:32.904064: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-04 12:17:34.189617: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-04 12:17:34.189867: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-04 12:17:34.189886: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.autograph.set_verbosity(0)\n",
    "tf.set_seed = 42\n",
    "epoch = 10\n",
    "max_vocab_length = 10000 # max number of words to have in our vocabulary\n",
    "max_length = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3  4  3 ... 25  3 25]\n",
      "[ 3 10  1 ...  3  3 24]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import data\n",
    "train_data, test_data = tf.keras.datasets.reuters.load_data(num_words=max_vocab_length)\n",
    "\n",
    "\"\"\"\n",
    "# Check the data shape\n",
    "print(\n",
    "    f\"Train_features : {train_features.shape} {train_features.dtype} \\n\" \n",
    "    f\"Train_labels : {train_labels.shape} {train_labels.dtype} \\n\" \n",
    "    f\"Test features : {test_features.shape} {test_features.dtype} \\n\" \n",
    "    f\"Test_labels : {test_labels.shape} {test_labels.dtype} \"\n",
    "    )\n",
    "\"\"\"\n",
    "print(train_data[1])\n",
    "print( test_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8982,), (8982,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feat_data, train_labels_data = train_data\n",
    "train_feat_data.shape , train_labels_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mcast(results, dtype\u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m      9\u001b[0m \u001b[39m# Vectorize and Normalize train and test to tensors with 10k columns\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m train_features_tf \u001b[39m=\u001b[39m vectorize_sequences(train_features)\n\u001b[1;32m     12\u001b[0m test_features_tf \u001b[39m=\u001b[39m vectorize_sequences(test_features)\n\u001b[1;32m     14\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtrain_features \u001b[39m\u001b[39m\"\u001b[39m, train_features_tf\u001b[39m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_features' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# VECTORIZE function\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=max_vocab_length):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return tf.cast(results, dtype= tf.float32)\n",
    "\n",
    "# Vectorize and Normalize train and test to tensors with 10k columns\n",
    "\n",
    "train_features_tf = vectorize_sequences(train_features)\n",
    "test_features_tf = vectorize_sequences(test_features)\n",
    "\n",
    "print(\"train_features \", train_features_tf.shape)\n",
    "print(\"test_Features \", test_features_tf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_hot_train_labels  (8982, 46)\n",
      "one_hot_test_labels  (2246, 46)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ONE HOT ENCODER of the labels\n",
    "\n",
    "one_hot_train_labels = tf.keras.utils.to_categorical(train_labels)\n",
    "one_hot_test_labels = tf.keras.utils.to_categorical(test_labels)\n",
    "\n",
    "print(\"one_hot_train_labels \", one_hot_train_labels.shape)\n",
    "print(\"one_hot_test_labels \", one_hot_test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : <PrefetchDataset element_spec=(TensorSpec(shape=(None, 10000), dtype=tf.float32, name=None), TensorSpec(shape=(None, 46), dtype=tf.float32, name=None))> \n",
      "Test : <PrefetchDataset element_spec=(TensorSpec(shape=(None, 10000), dtype=tf.float32, name=None), TensorSpec(shape=(None, 46), dtype=tf.float32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_features_tf, one_hot_train_labels))\n",
    "train_dataset =  train_dataset.shuffle(8982).batch(32).cache().prefetch(tf.data.AUTOTUNE)\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((test_features_tf,one_hot_test_labels))\n",
    "valid_dataset = valid_dataset.batch(32).cache().prefetch(tf.data.AUTOTUNE)\n",
    "print(f\"Train : {train_dataset} \\n\"\n",
    "      f\"Test : {valid_dataset}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Callbacks\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", # watch the val loss metric\n",
    "                                                  patience=5) # if val loss decreases for 3 epochs in a row, stop training\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",  \n",
    "                                                 factor=0.2, # multiply the learning rate by 0.2 (reduce by 5x)\n",
    "                                                 patience=3,\n",
    "                                                 verbose=1, # print out when learning rate goes down \n",
    "                                                 min_lr=1e-7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set random seed and create embedding layer (new embedding layer for each model)\n",
    "embedding_layers = layers.Embedding(input_dim=max_vocab_length,\n",
    "                                     output_dim=5,\n",
    "                                     embeddings_initializer=\"uniform\",\n",
    "                                     input_length = max_length,\n",
    "                                     name=\"embedding_layers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4_Bidirectional\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 10000)]           0         \n",
      "                                                                 \n",
      " embedding_layers (Embedding  (None, 10000, 5)         50000     \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 10000, 32)         832       \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 10000, 64)         10304     \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 10000, 128)        41088     \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 128)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 46)                5934      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 108,158\n",
      "Trainable params: 108,158\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build a Bidirectional RNN in TensorFlow\n",
    "inputs = layers.Input(shape=(max_vocab_length,))\n",
    "x = embedding_layers(inputs)\n",
    "x = layers.Conv1D(32, kernel_size=5, padding=\"same\", activation=\"relu\")(x)\n",
    "x = layers.Conv1D(64, kernel_size=5, padding=\"same\", activation=\"relu\")(x)\n",
    "x = layers.Conv1D(128, kernel_size=5, padding=\"same\", activation=\"relu\")(x)\n",
    "x = layers.GlobalAveragePooling1D()(x) # condense the output of our feature vector\n",
    "outputs = layers.Dense(46, activation=\"softmax\")(x)\n",
    "model= tf.keras.Model(inputs, outputs, name=\"model_4_Bidirectional\")\n",
    "\n",
    "# Compile\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# Get a summary of our bidirectional model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = datetime.now()\n",
    "# Fit the model (takes longer because of the bidirectional layers)\n",
    "model_history = model.fit(train_dataset,\n",
    "                           epochs=5,\n",
    "                           validation_data=valid_dataset,\n",
    "                           callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "end = datetime.now()\n",
    "print(f\"The time taken to fit the modle is {end - start}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.autograph.set_verbosity(0)\n",
    "tf.set_seed = 42\n",
    "epoch = 10\n",
    "max_vocab_length = 5000 # max number of words to have in our vocabulary\n",
    "max_length = 400\n",
    "\n",
    "# import data\n",
    "(train_features,train_labels), (test_features, test_labels) = tf.keras.datasets.reuters.load_data(num_words=max_vocab_length)\n",
    "\n",
    "# Check the data shape\n",
    "print(\n",
    "    f\"Train_features : {train_features.shape} {train_features.dtype} \\n\" \n",
    "    f\"Train_labels : {train_labels.shape} {train_labels.dtype} \\n\" \n",
    "    f\"Test features : {test_features.shape} {test_features.dtype} \\n\" \n",
    "    f\"Test_labels : {test_labels.shape} {test_labels.dtype} \"\n",
    "    )\n",
    "\n",
    "# VECTORIZE function\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=max_vocab_length):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return tf.cast(results, dtype= tf.float32)\n",
    "\n",
    "# Vectorize and Normalize train and test to tensors with 10k columns\n",
    "\n",
    "train_features_tf = vectorize_sequences(train_features)\n",
    "test_features_tf = vectorize_sequences(test_features)\n",
    "\n",
    "print(\"train_features \", train_features_tf.shape)\n",
    "print(\"test_Features \", test_features_tf.shape)\n",
    "\n",
    "# ONE HOT ENCODER of the labels\n",
    "\n",
    "one_hot_train_labels = tf.keras.utils.to_categorical(train_labels)\n",
    "one_hot_test_labels = tf.keras.utils.to_categorical(test_labels)\n",
    "\n",
    "print(\"one_hot_train_labels \", one_hot_train_labels.shape)\n",
    "print(\"one_hot_test_labels \", one_hot_test_labels.shape)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_features_tf, one_hot_train_labels))\n",
    "train_dataset =  train_dataset.shuffle(8982).batch(32).cache().prefetch(tf.data.AUTOTUNE)\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((test_features_tf,one_hot_test_labels))\n",
    "valid_dataset = valid_dataset.batch(32).cache().prefetch(tf.data.AUTOTUNE)\n",
    "print(f\"Train : {train_dataset} \\n\"\n",
    "      f\"Test : {valid_dataset}\")\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", # watch the val loss metric\n",
    "                                                  patience=5) # if val loss decreases for 3 epochs in a row, stop training\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",  \n",
    "                                                 factor=0.2, # multiply the learning rate by 0.2 (reduce by 5x)\n",
    "                                                 patience=3,\n",
    "                                                 verbose=1, # print out when learning rate goes down \n",
    "                                                 min_lr=1e-7)\n",
    "\n",
    "# Set random seed and create embedding layer (new embedding layer for each model)\n",
    "sentence_encoder_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "                                        input_shape=[], # shape of inputs coming to our model \n",
    "                                        dtype=tf.float32, # data type of inputs coming to the USE layer\n",
    "                                        trainable=False, # keep the pretrained weights (we'll create a feature extractor)\n",
    "                                        name=\"USE\")\n",
    "\n",
    "\n",
    "# Build a Bidirectional RNN in TensorFlow\n",
    "inputs = layers.Input(shape=(max_vocab_length,))\n",
    "x = sentence_encoder_layer(inputs)\n",
    "x = layers.Conv1D(32, kernel_size=5, padding=\"same\", activation=\"relu\")(x)\n",
    "x = layers.Conv1D(64, kernel_size=5, padding=\"same\", activation=\"relu\")(x)\n",
    "x = layers.Conv1D(128, kernel_size=5, padding=\"same\", activation=\"relu\")(x)\n",
    "x = layers.GlobalAveragePooling1D()(x) # condense the output of our feature vector\n",
    "outputs = layers.Dense(46, activation=\"softmax\")(x)\n",
    "model= tf.keras.Model(inputs, outputs, name=\"model_4_Bidirectional\")\n",
    "\n",
    "# Compile\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# Get a summary of our bidirectional model\n",
    "model.summary()\n",
    "\n",
    "start = datetime.now()\n",
    "# Fit the model (takes longer because of the bidirectional layers)\n",
    "model_history = model.fit(train_dataset,\n",
    "                           epochs=5,\n",
    "                           validation_data=valid_dataset,\n",
    "                           callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "end = datetime.now()\n",
    "print(f\"The time taken to fit the modle is {end - start}\")\n",
    "model.evaluate(valid_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Dec 19 2022, 17:35:49) [GCC 12.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2fcd31d782b6120f2993dc67dd18a19e8000177e7f61203562e17b1bd19b8f69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
